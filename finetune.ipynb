{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def load_model(name):\n",
    "    model_path = name\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        output_attentions=False\n",
    "    )\n",
    "    return model.cuda()\n",
    "tokenizer_path = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "\n",
    "models = [f\"./finetuned-scrumbled-wikitext2/checkpoint-{i}\" for i in range(500, 13501, 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-893c33a0074ecc88\n",
      "Found cached dataset csv (/home/sha43/.cache/huggingface/datasets/csv/default-893c33a0074ecc88/0.0.0)\n",
      "Loading cached processed dataset at /home/sha43/.cache/huggingface/datasets/csv/default-893c33a0074ecc88/0.0.0/cache-e8426cbdf3a0f8a0.arrow\n",
      "Using custom data configuration default-5194cde6cade5dda\n",
      "Found cached dataset csv (/home/sha43/.cache/huggingface/datasets/csv/default-5194cde6cade5dda/0.0.0)\n",
      "Loading cached processed dataset at /home/sha43/.cache/huggingface/datasets/csv/default-5194cde6cade5dda/0.0.0/cache-d9f72de92ccf7b02.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3468\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    res = tokenizer(examples[\"text\"], truncation=True, return_tensors=\"pt\", padding=True)\n",
    "    for elem in res:\n",
    "        res[elem] = res[elem].cuda()\n",
    "    return res\n",
    "\n",
    "train_ds = datasets.Dataset.from_csv(\"target_problem/train.csv\").map(preprocess_function, batched=True)\n",
    "test_ds = datasets.Dataset.from_csv(\"target_problem/test.csv\").map(preprocess_function, batched=True)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[0].argmax(axis=1)\n",
    "    print(predictions)\n",
    "    res = accuracy.compute(predictions=predictions, references=labels)\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./finetuned-scrumbled-wikitext2/checkpoint-500 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./finetuned-scrumbled-wikitext2/checkpoint-500 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/sha43/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3468\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 434\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [218/434 01:54 < 01:54, 1.88 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 68/217 04:00 < 08:54, 0.28 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3468\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m      6\u001b[0m     training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      7\u001b[0m         output_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mres/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[0;32m---> 26\u001b[0m     res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     27\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1532\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer.py:1867\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1864\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1867\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1869\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1870\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1871\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer.py:2115\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2109\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[1;32m   2110\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[1;32m   2111\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2112\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2113\u001b[0m             )\n\u001b[1;32m   2114\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2115\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer.py:2811\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2808\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2810\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2811\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2812\u001b[0m     eval_dataloader,\n\u001b[1;32m   2813\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2814\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2815\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2816\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2817\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2818\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2819\u001b[0m )\n\u001b[1;32m   2821\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2822\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   2823\u001b[0m     speed_metrics(\n\u001b[1;32m   2824\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2828\u001b[0m     )\n\u001b[1;32m   2829\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer.py:3026\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     logits \u001b[39m=\u001b[39m nested_numpify(preds_host)\n\u001b[0;32m-> 3026\u001b[0m     all_preds \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(all_preds, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m inputs_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     inputs_decode \u001b[39m=\u001b[39m nested_numpify(inputs_host)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;49;00m t, n \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;49;00m t, n \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(new_tensors)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39;49mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:121\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[1;32m    118\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type for concatenation: got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:98\u001b[0m, in \u001b[0;36mnumpy_pad_and_concatenate\u001b[0;34m(array1, array2, padding_index)\u001b[0m\n\u001b[1;32m     95\u001b[0m new_shape \u001b[39m=\u001b[39m (array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m array2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m array1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m     97\u001b[0m \u001b[39m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfull_like(array1, padding_index, shape\u001b[39m=\u001b[39;49mnew_shape)\n\u001b[1;32m     99\u001b[0m result[: array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], : array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m array1\n\u001b[1;32m    100\u001b[0m result[array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] :, : array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m array2\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mfull_like\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/diploma/lib/python3.9/site-packages/numpy/core/numeric.py:424\u001b[0m, in \u001b[0;36mfull_like\u001b[0;34m(a, fill_value, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39mReturn a full array with the same shape and type as a given array.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39m        [  0,   0, 255]]])\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    423\u001b[0m res \u001b[39m=\u001b[39m empty_like(a, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder, subok\u001b[39m=\u001b[39msubok, shape\u001b[39m=\u001b[39mshape)\n\u001b[0;32m--> 424\u001b[0m multiarray\u001b[39m.\u001b[39;49mcopyto(res, fill_value, casting\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39munsafe\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    425\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mcopyto\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "for model_name in models:\n",
    "    model = load_model(model_name).cuda()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"res/{model_name}\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_accumulation_steps=2,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=train_ds,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    res = trainer.train()\n",
    "    break\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46242d4576570c8c82d7fb56e00e670e22fb52831ef677247d21ad598b3c01cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

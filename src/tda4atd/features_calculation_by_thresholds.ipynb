{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from stats_count import *\n",
    "from grab_weights import grab_attention_weights, text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!env | grep CUDA_VISIBLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_amount  = 128 # The number of tokens to which the tokenized text is truncated / padded.\n",
    "stats_cap          = 500 # Max value that the feature can take. Is NOT applicable to Betty numbers.\n",
    "    \n",
    "layers_of_interest = [i for i in range(12)]  # Layers for which attention matrices and features on them are \n",
    "                                             # calculated. For calculating features on all layers, leave it be\n",
    "                                             # [i for i in range(12)].\n",
    "stats_name = \"s_e_v_c_b0b1\" # The set of topological features that will be count (see explanation below)\n",
    "\n",
    "thresholds_array = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75] # The set of thresholds\n",
    "thrs = len(thresholds_array)                           # (\"t\" in the paper)\n",
    "\n",
    "model_path = tokenizer_path = \"bert-base-uncased\"  \n",
    "\n",
    "# You can use either standard or fine-tuned BERT. If you want to use fine-tuned BERT to your current task, save the\n",
    "# model and the tokenizer with the commands tokenizer.save_pretrained(output_dir); \n",
    "# bert_classifier.save_pretrained(output_dir) into the same directory and insert the path to it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of stats_name parameter\n",
    "\n",
    "Currently, we implemented calculation of the following graphs features:\n",
    "* \"s\"    - amount of strongly connected components\n",
    "* \"w\"    - amount of weakly connected components\n",
    "* \"e\"    - amount of edges\n",
    "* \"v\"    - average vertex degree\n",
    "* \"c\"    - amount of (directed) simple cycles\n",
    "* \"b0b1\" - Betti numbers\n",
    "\n",
    "The variable stats_name contains a string with the names of the features, which you want to calculate. The format of the string is the following:\n",
    "\n",
    "\"stat_name + \"_\" + stat_name + \"_\" + stat_name + ...\"\n",
    "\n",
    "**For example**:\n",
    "\n",
    "`stats_name == \"s_w\"` means that the number of strongly and weakly connected components will be calculated\n",
    "\n",
    "`stats_name == \"b0b1\"` means that only the Betti numbers will be calculated\n",
    "\n",
    "`stats_name == \"b0b1_c\"` means that Betti numbers and the number of simple cycles will be calculated\n",
    "\n",
    "e.t.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = \"test_5k\"           # .csv file with the texts, for which we count topological features\n",
    "input_dir = \"small_gpt_web/\"  # Name of the directory with .csv file\n",
    "output_dir = \"small_gpt_web/\" # Name of the directory with calculations results\n",
    "\n",
    "prefix = output_dir + subset\n",
    "\n",
    "r_file     = output_dir + 'attentions/' + subset  + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers_MAX_LEN_\" + \\\n",
    "             str(max_tokens_amount) + \"_\" + model_path.split(\"/\")[-1]\n",
    "# Name of the file for attention matrices weights\n",
    "\n",
    "stats_file = output_dir + 'features/' + subset + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers_\" + stats_name \\\n",
    "             + \"_lists_array_\" + str(thrs) + \"_thrs_MAX_LEN_\" + str(max_tokens_amount) + \\\n",
    "             \"_\" + model_path.split(\"/\")[-1] + '.npy'\n",
    "# Name of the file for topological features array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small_gpt_web/features/test_5k_all_heads_12_layers_s_e_v_c_b0b1_lists_array_6_thrs_MAX_LEN_128_bert-base-uncased.npy'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small_gpt_web/attentions/test_5k_all_heads_12_layers_MAX_LEN_128_bert-base-uncased'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".csv file must contain the column with the name **sentence** with the texts. It can also contain the column **labels**, which will be needed for testing. Any other arbitrary columns will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(input_dir + subset + \".csv\").reset_index(drop=True)\n",
    "except:\n",
    "    #data = pd.read_csv(input_dir + subset + \".tsv\", delimiter=\"\\t\")\n",
    "    data = pd.read_csv(input_dir + subset + \".tsv\", delimiter=\"\\t\", header=None)\n",
    "    data.columns = [\"0\", \"labels\", \"2\", \"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>ended</th>\n",
       "      <th>length</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4722</td>\n",
       "      <td>259722</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>The Learning Co.\\n\\nDeveloped by\\n\\nThe Learni...</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2757</td>\n",
       "      <td>257813</td>\n",
       "      <td>True</td>\n",
       "      <td>563</td>\n",
       "      <td>Bush doubles down on foreign policy on Saturda...</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2194</td>\n",
       "      <td>257194</td>\n",
       "      <td>True</td>\n",
       "      <td>62</td>\n",
       "      <td>Here are six interesting things you need to kn...</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>817</td>\n",
       "      <td>255817</td>\n",
       "      <td>True</td>\n",
       "      <td>293</td>\n",
       "      <td>Introduction\\n\\nWe would like to thank Antec f...</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3886</td>\n",
       "      <td>258886</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>ELKRIDGE, Md.—A group called \"Muslims for Trum...</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id  ended  length  \\\n",
       "0        4722  259722   True     231   \n",
       "1        2757  257813   True     563   \n",
       "2        2194  257194   True      62   \n",
       "3         817  255817   True     293   \n",
       "4        3886  258886  False    1024   \n",
       "\n",
       "                                            sentence      label  \n",
       "0  The Learning Co.\\n\\nDeveloped by\\n\\nThe Learni...    natural  \n",
       "1  Bush doubles down on foreign policy on Saturda...  generated  \n",
       "2  Here are six interesting things you need to kn...    natural  \n",
       "3  Introduction\\n\\nWe would like to thank Antec f...    natural  \n",
       "4  ELKRIDGE, Md.—A group called \"Muslims for Trum...    natural  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of words in example: 2723.5124\n",
      "Max. amount of words in example: 6151\n",
      "Min. amount of words in example: 34\n"
     ]
    }
   ],
   "source": [
    "sentences = data['sentence']\n",
    "print(\"Average amount of words in example:\", \\\n",
    "      np.mean(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), data['sentence'])))))\n",
    "print(\"Max. amount of words in example:\", \\\n",
    "      np.max(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), data['sentence'])))))\n",
    "print(\"Min. amount of words in example:\", \\\n",
    "      np.min(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), data['sentence'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_length(batch_texts):\n",
    "    inputs = tokenizer.batch_encode_plus(batch_texts,\n",
    "       return_tensors='pt',\n",
    "       add_special_tokens=True,\n",
    "       max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "       pad_to_max_length=True,         # Pad sentence to max length\n",
    "       truncation=True\n",
    "    )\n",
    "    inputs = inputs['input_ids'].numpy()\n",
    "    n_tokens = []\n",
    "    indexes = np.argwhere(inputs == tokenizer.pad_token_id)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        ids = indexes[(indexes == i)[:, 0]]\n",
    "        if not len(ids):\n",
    "            n_tokens.append(MAX_LEN)\n",
    "        else:\n",
    "            n_tokens.append(ids[0, 1])\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_tokens_amount\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenizer_length'] = get_token_length(data['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>ended</th>\n",
       "      <th>length</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenizer_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4722</td>\n",
       "      <td>259722</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>The Learning Co.\\n\\nDeveloped by\\n\\nThe Learni...</td>\n",
       "      <td>natural</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2757</td>\n",
       "      <td>257813</td>\n",
       "      <td>True</td>\n",
       "      <td>563</td>\n",
       "      <td>Bush doubles down on foreign policy on Saturda...</td>\n",
       "      <td>generated</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2194</td>\n",
       "      <td>257194</td>\n",
       "      <td>True</td>\n",
       "      <td>62</td>\n",
       "      <td>Here are six interesting things you need to kn...</td>\n",
       "      <td>natural</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>817</td>\n",
       "      <td>255817</td>\n",
       "      <td>True</td>\n",
       "      <td>293</td>\n",
       "      <td>Introduction\\n\\nWe would like to thank Antec f...</td>\n",
       "      <td>natural</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3886</td>\n",
       "      <td>258886</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>ELKRIDGE, Md.—A group called \"Muslims for Trum...</td>\n",
       "      <td>natural</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1472</td>\n",
       "      <td>256472</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>Occasionally, we come across interesting scena...</td>\n",
       "      <td>natural</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>326</td>\n",
       "      <td>255337</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>Providing insight not only into the memes that...</td>\n",
       "      <td>generated</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>3862</td>\n",
       "      <td>258862</td>\n",
       "      <td>True</td>\n",
       "      <td>339</td>\n",
       "      <td>Each year, MONEY digs into enrollment data and...</td>\n",
       "      <td>natural</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2862</td>\n",
       "      <td>257862</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>Grounding of the Queen Elizabeth 2 (response) ...</td>\n",
       "      <td>natural</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2439</td>\n",
       "      <td>257487</td>\n",
       "      <td>True</td>\n",
       "      <td>506</td>\n",
       "      <td>Route 128\\n\\nTaxi Use\\n\\nCan 2,245 miles by 2....</td>\n",
       "      <td>generated</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      id  ended  length  \\\n",
       "0           4722  259722   True     231   \n",
       "1           2757  257813   True     563   \n",
       "2           2194  257194   True      62   \n",
       "3            817  255817   True     293   \n",
       "4           3886  258886  False    1024   \n",
       "...          ...     ...    ...     ...   \n",
       "4995        1472  256472  False    1024   \n",
       "4996         326  255337  False    1024   \n",
       "4997        3862  258862   True     339   \n",
       "4998        2862  257862  False    1024   \n",
       "4999        2439  257487   True     506   \n",
       "\n",
       "                                               sentence      label  \\\n",
       "0     The Learning Co.\\n\\nDeveloped by\\n\\nThe Learni...    natural   \n",
       "1     Bush doubles down on foreign policy on Saturda...  generated   \n",
       "2     Here are six interesting things you need to kn...    natural   \n",
       "3     Introduction\\n\\nWe would like to thank Antec f...    natural   \n",
       "4     ELKRIDGE, Md.—A group called \"Muslims for Trum...    natural   \n",
       "...                                                 ...        ...   \n",
       "4995  Occasionally, we come across interesting scena...    natural   \n",
       "4996  Providing insight not only into the memes that...  generated   \n",
       "4997  Each year, MONEY digs into enrollment data and...    natural   \n",
       "4998  Grounding of the Queen Elizabeth 2 (response) ...    natural   \n",
       "4999  Route 128\\n\\nTaxi Use\\n\\nCan 2,245 miles by 2....  generated   \n",
       "\n",
       "      tokenizer_length  \n",
       "0                  128  \n",
       "1                  128  \n",
       "2                   71  \n",
       "3                  128  \n",
       "4                  128  \n",
       "...                ...  \n",
       "4995               128  \n",
       "4996               128  \n",
       "4997               128  \n",
       "4998               128  \n",
       "4999               128  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens_array = data['tokenizer_length'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading **BERT** and tokenizers using **transformers** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 10 # batch size\n",
    "number_of_batches = ceil(len(data['sentence']) / batch_size)\n",
    "DUMP_SIZE = 100 # number of batches to be dumped\n",
    "batched_sentences = np.array_split(data['sentence'].values, number_of_batches)\n",
    "number_of_files = ceil(number_of_batches / DUMP_SIZE)\n",
    "adj_matricies = []\n",
    "adj_filenames = []\n",
    "assert number_of_batches == len(batched_sentences) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "model = model.to(device)\n",
    "MAX_LEN = max_tokens_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved.\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm(range(number_of_batches), desc=\"Weights calc\"): \n",
    "#     attention_w = grab_attention_weights(model, tokenizer, batched_sentences[i], max_tokens_amount, device)\n",
    "#     # sample X layer X head X n_token X n_token\n",
    "#     adj_matricies.append(attention_w)\n",
    "#     if (i+1) % DUMP_SIZE == 0: # dumping\n",
    "#         print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "#         adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "#         print(\"Concatenated\")\n",
    "#         adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "#         filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)) + \"of\" + str(number_of_files) + '.npy'\n",
    "#         print(f\"Saving weights to : {filename}\")\n",
    "#         adj_filenames.append(filename)\n",
    "#         np.save(filename, adj_matricies)\n",
    "#         adj_matricies = []\n",
    "        \n",
    "if len(adj_matricies):\n",
    "    filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)) + \"of\" + str(number_of_files) + '.npy'\n",
    "    print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "    adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "    print(\"Concatenated\")\n",
    "    adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "    print(f\"Saving weights to : {filename}\")\n",
    "    np.save(filename, adj_matricies)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating topological features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'e', 'v', 'c', 'b0b1']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_name.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small_gpt_web/attentions/test_5k_all_heads_12_layers_MAX_LEN_128_bert-base-uncased_part1of5.npy',\n",
       " 'small_gpt_web/attentions/test_5k_all_heads_12_layers_MAX_LEN_128_bert-base-uncased_part2of5.npy',\n",
       " 'small_gpt_web/attentions/test_5k_all_heads_12_layers_MAX_LEN_128_bert-base-uncased_part3of5.npy',\n",
       " 'small_gpt_web/attentions/test_5k_all_heads_12_layers_MAX_LEN_128_bert-base-uncased_part4of5.npy',\n",
       " 'small_gpt_web/attentions/test_5k_all_heads_12_layers_MAX_LEN_128_bert-base-uncased_part5of5.npy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "adj_filenames = [\n",
    "    output_dir + 'attentions/' + filename \n",
    "    for filename in os.listdir(output_dir + 'attentions/') if r_file in (output_dir + 'attentions/' + filename)\n",
    "]\n",
    "# sorted by part number\n",
    "adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is calculated in \"f(v)\". You can add any other function from the array with vertex degrees.\n",
    "\n",
    "def function_for_v(list_of_v_degrees_of_graph):\n",
    "    return sum(map(lambda x: np.sqrt(x*x), list_of_v_degrees_of_graph))\n",
    "\n",
    "def split_matricies_and_lengths(adj_matricies, ntokens_array, num_of_workers):\n",
    "    splitted_adj_matricies = np.array_split(adj_matricies, num_of_workers)\n",
    "    splitted_ntokens = np.array_split(ntokens_array, num_of_workers)\n",
    "    assert all([len(m)==len(n) for m, n in zip(splitted_adj_matricies, splitted_ntokens)]), \"Split is not valid!\"\n",
    "    return zip(splitted_adj_matricies, splitted_ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_workers = 20\n",
    "pool = Pool(num_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 141699.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Вычисление признаков:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [09:26<00:00, 47.24s/it]\n",
      "100%|██████████| 12/12 [09:40<00:00, 48.38s/it]\n",
      "100%|██████████| 12/12 [09:33<00:00, 47.77s/it]\n",
      "100%|██████████| 12/12 [09:50<00:00, 49.25s/it]\n",
      "100%|██████████| 12/12 [09:48<00:00, 49.01s/it]\n",
      "100%|██████████| 12/12 [09:53<00:00, 49.49s/it]\n",
      "100%|██████████| 12/12 [09:56<00:00, 49.72s/it]\n",
      "100%|██████████| 12/12 [09:59<00:00, 49.97s/it]\n",
      "100%|██████████| 12/12 [09:51<00:00, 49.32s/it]\n",
      "100%|██████████| 12/12 [09:57<00:00, 49.79s/it]\n",
      "100%|██████████| 12/12 [09:54<00:00, 49.54s/it]\n",
      "100%|██████████| 12/12 [10:02<00:00, 50.24s/it]\n",
      "100%|██████████| 12/12 [09:51<00:00, 49.27s/it]\n",
      "100%|██████████| 12/12 [09:53<00:00, 49.50s/it]\n",
      "100%|██████████| 12/12 [09:56<00:00, 49.69s/it]\n",
      "100%|██████████| 12/12 [09:47<00:00, 48.96s/it]\n",
      "100%|██████████| 12/12 [09:47<00:00, 49.00s/it]\n",
      "100%|██████████| 12/12 [09:52<00:00, 49.38s/it]\n",
      "100%|██████████| 12/12 [09:51<00:00, 49.31s/it]\n",
      "100%|██████████| 12/12 [09:55<00:00, 49.67s/it]\n",
      "Вычисление признаков:  20%|██        | 1/5 [10:14<40:56, 614.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [08:48<00:00, 44.01s/it]\n",
      "100%|██████████| 12/12 [08:56<00:00, 44.68s/it]\n",
      "100%|██████████| 12/12 [08:58<00:00, 44.88s/it]\n",
      "100%|██████████| 12/12 [09:08<00:00, 45.69s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.33s/it]\n",
      "100%|██████████| 12/12 [08:59<00:00, 45.00s/it]\n",
      "100%|██████████| 12/12 [09:12<00:00, 46.05s/it]\n",
      "100%|██████████| 12/12 [09:06<00:00, 45.56s/it]\n",
      "100%|██████████| 12/12 [09:13<00:00, 46.10s/it]\n",
      "100%|██████████| 12/12 [09:21<00:00, 46.77s/it]\n",
      "100%|██████████| 12/12 [08:58<00:00, 44.90s/it]\n",
      "100%|██████████| 12/12 [09:13<00:00, 46.17s/it]\n",
      "100%|██████████| 12/12 [08:57<00:00, 44.76s/it]\n",
      "100%|██████████| 12/12 [09:05<00:00, 45.49s/it]\n",
      "100%|██████████| 12/12 [09:12<00:00, 46.07s/it]\n",
      "100%|██████████| 12/12 [08:54<00:00, 44.58s/it]\n",
      "100%|██████████| 12/12 [09:14<00:00, 46.19s/it]\n",
      "100%|██████████| 12/12 [09:06<00:00, 45.54s/it]\n",
      "100%|██████████| 12/12 [09:10<00:00, 45.90s/it]\n",
      "100%|██████████| 12/12 [09:07<00:00, 45.64s/it]\n",
      "Вычисление признаков:  40%|████      | 2/5 [20:04<29:59, 599.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [09:01<00:00, 45.09s/it]\n",
      "100%|██████████| 12/12 [09:06<00:00, 45.52s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.29s/it]\n",
      "100%|██████████| 12/12 [09:07<00:00, 45.64s/it]\n",
      "100%|██████████| 12/12 [09:04<00:00, 45.42s/it]\n",
      "100%|██████████| 12/12 [09:11<00:00, 45.94s/it]\n",
      "100%|██████████| 12/12 [09:10<00:00, 45.92s/it]\n",
      "100%|██████████| 12/12 [08:57<00:00, 44.78s/it]\n",
      "100%|██████████| 12/12 [09:17<00:00, 46.45s/it]\n",
      "100%|██████████| 12/12 [09:08<00:00, 45.68s/it]\n",
      "100%|██████████| 12/12 [09:13<00:00, 46.12s/it]\n",
      "100%|██████████| 12/12 [09:20<00:00, 46.70s/it]\n",
      "100%|██████████| 12/12 [09:16<00:00, 46.37s/it]\n",
      "100%|██████████| 12/12 [09:13<00:00, 46.10s/it]\n",
      "100%|██████████| 12/12 [09:14<00:00, 46.22s/it]\n",
      "100%|██████████| 12/12 [09:08<00:00, 45.72s/it]\n",
      "100%|██████████| 12/12 [09:14<00:00, 46.17s/it]\n",
      "100%|██████████| 12/12 [09:11<00:00, 45.93s/it]\n",
      "100%|██████████| 12/12 [09:17<00:00, 46.46s/it]\n",
      "100%|██████████| 12/12 [09:18<00:00, 46.50s/it]\n",
      "Вычисление признаков:  60%|██████    | 3/5 [29:39<19:37, 588.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [08:42<00:00, 43.57s/it]\n",
      "100%|██████████| 12/12 [08:50<00:00, 44.21s/it]\n",
      "100%|██████████| 12/12 [08:54<00:00, 44.51s/it]\n",
      "100%|██████████| 12/12 [09:00<00:00, 45.00s/it]\n",
      "100%|██████████| 12/12 [09:02<00:00, 45.25s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.25s/it]\n",
      "100%|██████████| 12/12 [09:04<00:00, 45.38s/it]\n",
      "100%|██████████| 12/12 [09:02<00:00, 45.20s/it]\n",
      "100%|██████████| 12/12 [09:04<00:00, 45.42s/it]\n",
      "100%|██████████| 12/12 [09:00<00:00, 45.05s/it]\n",
      "100%|██████████| 12/12 [09:02<00:00, 45.23s/it]\n",
      "100%|██████████| 12/12 [09:06<00:00, 45.53s/it]\n",
      "100%|██████████| 12/12 [09:02<00:00, 45.24s/it]\n",
      "100%|██████████| 12/12 [08:55<00:00, 44.63s/it]\n",
      "100%|██████████| 12/12 [09:00<00:00, 45.06s/it]\n",
      "100%|██████████| 12/12 [09:05<00:00, 45.49s/it]\n",
      "100%|██████████| 12/12 [09:00<00:00, 45.05s/it]\n",
      "100%|██████████| 12/12 [09:01<00:00, 45.15s/it]\n",
      "100%|██████████| 12/12 [09:06<00:00, 45.55s/it]\n",
      "100%|██████████| 12/12 [08:58<00:00, 44.91s/it]\n",
      "Вычисление признаков:  80%|████████  | 4/5 [39:01<09:38, 578.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [08:39<00:00, 43.26s/it]\n",
      "100%|██████████| 12/12 [08:41<00:00, 43.45s/it]\n",
      "100%|██████████| 12/12 [08:52<00:00, 44.38s/it]\n",
      "100%|██████████| 12/12 [08:48<00:00, 44.04s/it]\n",
      "100%|██████████| 12/12 [08:55<00:00, 44.64s/it]\n",
      "100%|██████████| 12/12 [08:47<00:00, 44.00s/it]\n",
      "100%|██████████| 12/12 [08:51<00:00, 44.28s/it]\n",
      "100%|██████████| 12/12 [09:01<00:00, 45.11s/it]\n",
      "100%|██████████| 12/12 [09:01<00:00, 45.13s/it]\n",
      "100%|██████████| 12/12 [08:53<00:00, 44.42s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.28s/it]\n",
      "100%|██████████| 12/12 [09:01<00:00, 45.09s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.31s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.33s/it]\n",
      "100%|██████████| 12/12 [08:56<00:00, 44.67s/it]\n",
      "100%|██████████| 12/12 [09:00<00:00, 45.03s/it]\n",
      "100%|██████████| 12/12 [08:55<00:00, 44.65s/it]\n",
      "100%|██████████| 12/12 [08:57<00:00, 44.83s/it]\n",
      "100%|██████████| 12/12 [08:55<00:00, 44.66s/it]\n",
      "100%|██████████| 12/12 [08:59<00:00, 44.98s/it]\n",
      "Вычисление признаков: 100%|██████████| 5/5 [48:23<00:00, 580.66s/it]\n"
     ]
    }
   ],
   "source": [
    "stats_tuple_lists_array = []\n",
    "for i, filename in enumerate(tqdm(adj_filenames, desc='Вычисление признаков')):\n",
    "    print(i)\n",
    "    adj_matricies = np.load(filename, allow_pickle=True)\n",
    "    ntokens = ntokens_array[i*batch_size*DUMP_SIZE : (i+1)*batch_size*DUMP_SIZE]\n",
    "    splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers)\n",
    "    args = [(m, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap) for m, ntokens in splitted]\n",
    "    stats_tuple_lists_array_part = pool.starmap(\n",
    "        count_top_stats, args\n",
    "    )\n",
    "    stats_tuple_lists_array.append(np.concatenate([_ for _ in stats_tuple_lists_array_part], axis=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_tuple_lists_array = np.concatenate(stats_tuple_lists_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 6, 5000, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_tuple_lists_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import inf\n",
    "\n",
    "np.sum(stats_tuple_lists_array[stats_tuple_lists_array == -inf]) + \\\n",
    "np.sum(stats_tuple_lists_array[stats_tuple_lists_array == inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small_gpt_web/features/test_5k_all_heads_12_layers_s_e_v_c_b0b1_lists_array_6_thrs_MAX_LEN_128_bert-base-uncased.npy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(stats_file, stats_tuple_lists_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the size of features matrices:\n",
    "\n",
    "Layers amount **Х** Heads amount **Х** Features amount **X** Examples amount **Х** Thresholds amount\n",
    "\n",
    "**For example**:\n",
    "\n",
    "`stats_name == \"s_w\"` => `Features amount == 2`\n",
    "\n",
    "`stats_name == \"b0b1\"` => `Features amount == 2`\n",
    "\n",
    "`stats_name == \"b0b1_c\"` => `Features amount == 3`\n",
    "\n",
    "e.t.c.\n",
    "\n",
    "`thresholds_array == [0.025, 0.05, 0.1, 0.25, 0.5, 0.75]` => `Thresholds amount == 6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 6, 5000, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_tuple_lists_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "46242d4576570c8c82d7fb56e00e670e22fb52831ef677247d21ad598b3c01cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

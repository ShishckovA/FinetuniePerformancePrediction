{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from tda4atd.stats_count import *\n",
    "from tda4atd.grab_weights import grab_attention_weights, text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_amount  = 512 # The number of tokens to which the tokenized text is truncated / padded.\n",
    "stats_cap          = 500 # Max value that the feature can take. Is NOT applicable to Betty numbers.\n",
    "    \n",
    "layers_of_interest = [i for i in range(12)]  # Layers for which attention matrices and features on them are \n",
    "                                             # calculated. For calculating features on all layers, leave it be\n",
    "                                             # [i for i in range(12)].\n",
    "stats_name = \"s_e_v_c_b0b1\" # The set of topological features that will be count (see explanation below)\n",
    "\n",
    "thresholds_array = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75] # The set of thresholds\n",
    "thrs = len(thresholds_array)                           # (\"t\" in the paper)\n",
    "\n",
    "model_path = tokenizer_path = \"bert-base-uncased\"  \n",
    "\n",
    "# You can use either standard or fine-tuned BERT. If you want to use fine-tuned BERT to your current task, save the\n",
    "# model and the tokenizer with the commands tokenizer.save_pretrained(output_dir); \n",
    "# bert_classifier.save_pretrained(output_dir) into the same directory and insert the path to it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"reviews_train\"           # .csv file with the texts, for which we count topological features\n",
    "name = \"senteval\"           # .csv file with the texts, for which we count topological features\n",
    "if name == \"revies_train\":\n",
    "    output_dir = \"target_problem/\"\n",
    "else:\n",
    "    output_dir = \"senteval_proc_data/\" # Name of the directory with calculations results\n",
    "\n",
    "prefix = output_dir + name\n",
    "\n",
    "r_file     = output_dir + 'attentions/' + name  + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers_MAX_LEN_\" + \\\n",
    "             str(max_tokens_amount) + \"_\" + model_path.split(\"/\")[-1]\n",
    "# Name of the file for attention matrices weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_tokens_amount\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "tokenize_batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_length(batch_texts):\n",
    "    for elem in batch_texts:\n",
    "        if not isinstance(elem, str):\n",
    "            print(elem)\n",
    "            break\n",
    "        # break\n",
    "    all_inputs = []\n",
    "    for i in tqdm(range(0, len(batch_texts), tokenize_batch_size)):\n",
    "        inputs_cur = tokenizer.batch_encode_plus(batch_texts[i:i+tokenize_batch_size],\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            truncation=True,\n",
    "        )['input_ids']\n",
    "        all_inputs.append(inputs_cur)\n",
    "    print(\"done\")\n",
    "    inputs = np.concatenate(all_inputs)\n",
    "    print(inputs.shape)\n",
    "    n_tokens = []\n",
    "    for i in tqdm(range(inputs.shape[0])):\n",
    "        good = np.argwhere(inputs[i] == tokenizer.pad_token_id)\n",
    "        if not len(good):\n",
    "            n_tokens.append(MAX_LEN)\n",
    "        else:\n",
    "            n_tokens.append(np.min(good))\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sent_analyze():\n",
    "    data = pd.read_csv(\"target_problem/reviews_train.csv\").reset_index(drop=True)\n",
    "    data = data[~data[\"cleaned_review\"].isna()]\n",
    "    data['tokenizer_length'] = get_token_length(data['cleaned_review'].values)\n",
    "    ntokens_array = data['tokenizer_length'].values\n",
    "    return data[\"cleaned_review\"].values, ntokens_array\n",
    "\n",
    "def read_sent_eval():\n",
    "    data = []\n",
    "    with open(\"SentEval/data/all_data.txt\") as fin:\n",
    "        for line in fin:\n",
    "            data.append(line)\n",
    "    tokenizer_length = get_token_length(data)\n",
    "    print(len(data))\n",
    "    return data, tokenizer_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SentEval/data/all_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_428466/2202299547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_sent_analyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"senteval\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_sent_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_428466/1086963175.py\u001b[0m in \u001b[0;36mread_sent_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_sent_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SentEval/data/all_data.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SentEval/data/all_data.txt'"
     ]
    }
   ],
   "source": [
    "if name == \"reviews_train\":\n",
    "    texts, ntokens_array = read_sent_analyze()\n",
    "if name == \"senteval\":\n",
    "    texts, ntokens_array = read_sent_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_428466/3221171042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;31m# batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatched_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnumber_of_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDUMP_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# number of batches to be dumped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 64 # batch size\n",
    "batched_sentences = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "number_of_batches = len(batched_sentences)\n",
    "DUMP_SIZE = 10 # number of batches to be dumped\n",
    "number_of_files = ceil(number_of_batches / DUMP_SIZE)\n",
    "adj_matricies = []\n",
    "adj_filenames = []\n",
    "assert number_of_batches == len(batched_sentences) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "model = model.to(device)\n",
    "MAX_LEN = max_tokens_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4972cf84ac847d5967b437e6f4b687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Weights calc:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part1of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part2of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part3of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part4of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part5of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part6of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part7of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part8of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part9of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part10of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part11of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part12of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part13of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part14of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part15of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part16of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part17of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part18of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part19of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part20of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part21of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part22of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part23of24.npy\n",
      "Saving: shape (12, 64, 1, 512, 512)\n",
      "Concatenated\n",
      "Saving weights to : senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part24of24.npy\n",
      "Results saved.\n"
     ]
    }
   ],
   "source": [
    "use_first_head = True\n",
    "\n",
    "for i in tqdm(range(number_of_batches), desc=\"Weights calc\"): \n",
    "    attention_w = grab_attention_weights(model, tokenizer, batched_sentences[i], max_tokens_amount, device)\n",
    "    if use_first_head:\n",
    "        attention_w = attention_w[:, :, :1, :, :]\n",
    "    # sample X layer X head X n_token X n_token\n",
    "    adj_matricies.append(attention_w)\n",
    "    if (i+1) % DUMP_SIZE == 0: # dumping\n",
    "        print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "        adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "        print(\"Concatenated\")\n",
    "        adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "        filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)) + \"of\" + str(number_of_files) + '.npy'\n",
    "        print(f\"Saving weights to : {filename}\")\n",
    "        adj_filenames.append(filename)\n",
    "        np.save(filename, adj_matricies)\n",
    "        adj_matricies = []\n",
    "        \n",
    "if len(adj_matricies):\n",
    "    filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)) + \"of\" + str(number_of_files) + '.npy'\n",
    "    print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "    adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "    print(\"Concatenated\")\n",
    "    adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "    print(f\"Saving weights to : {filename}\")\n",
    "    np.save(filename, adj_matricies)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'e', 'v', 'c', 'b0b1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_name.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part1of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part2of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part3of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part4of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part5of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part6of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part7of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part8of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part9of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part10of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part11of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part12of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part13of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part14of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part15of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part16of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part17of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part18of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part19of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part20of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part21of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part22of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part23of24.npy',\n",
       " 'senteval_proc_data/attentions/senteval_all_heads_12_layers_MAX_LEN_512_bert-base-uncased_part24of24.npy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "adj_filenames = [\n",
    "    output_dir + 'attentions/' + filename \n",
    "    for filename in os.listdir(output_dir + 'attentions/') if r_file in (output_dir + 'attentions/' + filename)\n",
    "]\n",
    "# sorted by part number\n",
    "adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is calculated in \"f(v)\". You can add any other function from the array with vertex degrees.\n",
    "\n",
    "def function_for_v(list_of_v_degrees_of_graph):\n",
    "    return sum(map(lambda x: np.sqrt(x*x), list_of_v_degrees_of_graph))\n",
    "\n",
    "def split_matricies_and_lengths(adj_matricies, ntokens_array, num_of_workers):\n",
    "    splitted_adj_matricies = np.array_split(adj_matricies, num_of_workers)\n",
    "    splitted_ntokens = np.array_split(ntokens_array, num_of_workers)\n",
    "    print(len(adj_matricies), len(ntokens_array))\n",
    "    print([(len(m), len(n)) for m, n in zip(splitted_adj_matricies, splitted_ntokens)])\n",
    "    assert all([len(m)==len(n) for m, n in zip(splitted_adj_matricies, splitted_ntokens)]), \"Split is not valid!\"\n",
    "    return zip(splitted_adj_matricies, splitted_ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_workers = 20\n",
    "pool = Pool(num_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bbaa8aca5e4a30951fdae845d35dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Вычисление признаков:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "1\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "2\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "3\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "4\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "5\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "6\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "7\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "8\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "9\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "10\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "11\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "12\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "13\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "14\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "15\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "16\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "17\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "18\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "19\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "20\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "21\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "22\n",
      "loaded\n",
      "ntokens count\n",
      "640 640\n",
      "[(32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32), (32, 32)]\n",
      "split_matricies_and_lengths\n",
      "23\n",
      "loaded\n",
      "ntokens count\n",
      "280 280\n",
      "[(14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14), (14, 14)]\n",
      "split_matricies_and_lengths\n"
     ]
    }
   ],
   "source": [
    "stats_tuple_lists_array = []\n",
    "for i, filename in enumerate(tqdm(adj_filenames, desc='Вычисление признаков')):\n",
    "    print(i)\n",
    "    adj_matricies = np.load(filename, allow_pickle=True)\n",
    "    print(\"loaded\")\n",
    "    ntokens = ntokens_array[i*batch_size*DUMP_SIZE : (i+1)*batch_size*DUMP_SIZE]\n",
    "    print(\"ntokens count\")\n",
    "    splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers)\n",
    "    print(\"split_matricies_and_lengths\")\n",
    "    args = [(m, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap) for m, ntokens in splitted]\n",
    "    stats_tuple_lists_array_part = pool.starmap(\n",
    "        count_top_stats, args\n",
    "    )\n",
    "    stats_tuple_lists_array.append(np.concatenate([_ for _ in stats_tuple_lists_array_part], axis=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_tuple_lists_array = np.concatenate(stats_tuple_lists_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1, 6, 15000, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_tuple_lists_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.npy\", \"wb\") as f:\n",
    "    np.save(f, stats_tuple_lists_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "46242d4576570c8c82d7fb56e00e670e22fb52831ef677247d21ad598b3c01cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

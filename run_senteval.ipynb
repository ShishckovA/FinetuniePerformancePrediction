{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import senteval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = tokenizer_path = \"bert-base-uncased\"  \n",
    "model = BertForSequenceClassification.from_pretrained(model_path, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CR': {'devacc': 87.36, 'acc': 87.6, 'ndev': 3775, 'ntest': 3775}, 'MR': {'devacc': 82.42, 'acc': 82.53, 'ndev': 10662, 'ntest': 10662}, 'MPQA': {'devacc': 88.41, 'acc': 88.34, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.64, 'acc': 95.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.24, 'acc': 86.77, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 46.23, 'acc': 45.52, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 86.79, 'acc': 91.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.16, 'acc': 70.09, 'f1': 79.18, 'ndev': 4076, 'ntest': 1725}, 'SNLI': {'devacc': 61.88, 'acc': 61.89, 'ndev': 9842, 'ntest': 9824}, 'SICKEntailment': {'devacc': 73.0, 'acc': 71.28, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7012727576920597, 'pearson': 0.6963877554642018, 'spearman': 0.6423404595047993, 'mse': 0.5256912964540551, 'yhat': array([3.21720952, 4.01886619, 2.35996623, ..., 3.29405314, 4.38710526,\n",
      "       4.20570849]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5747730496976102, 'pearson': 0.5051662018917707, 'spearman': 0.5040132721214705, 'mse': 1.935640326383409, 'yhat': array([2.55525455, 1.08764994, 2.31400733, ..., 4.42909176, 2.41024006,\n",
      "       4.35939654]), 'ndev': 1500, 'ntest': 1379}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import senteval\n",
    "def prepare(params, samples):\n",
    "    return \n",
    "\n",
    "def batcher(_, batch):\n",
    "    batch = [\" \".join(sts) for sts in batch]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            truncation=True)\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(\"cuda\")\n",
    "    res = model(**inputs)[\"hidden_states\"]\n",
    "    res = res[-1][:, 0].detach().cpu().numpy()\n",
    "    return res\n",
    "\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "model = model.to(device)\n",
    "PATH_TO_DATA = \"SentEval/data\"\n",
    "PARAMS = {'task_path': PATH_TO_DATA, 'usepytorch': False, 'kfold': 10, 'batch_size': 128}\n",
    "PARAMS['classifier'] = {'max_itaser': 10000, 'nhid': 768}\n",
    "transfer_tasks = ['CR', 'MR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC', 'SNLI', 'SICKEntailment', 'SICKRelatedness', 'STSBenchmark']\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    se = senteval.engine.SE(PARAMS, batcher, prepare)\n",
    "    results = se.eval(transfer_tasks)\n",
    "# print('Time took on task %s : %.1f. seconds' % (TASK, end - start))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46242d4576570c8c82d7fb56e00e670e22fb52831ef677247d21ad598b3c01cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

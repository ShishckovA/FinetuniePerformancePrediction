{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from tda4atd.stats_count import *\n",
    "from tda4atd.grab_weights import grab_attention_weights, text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!env | grep CUDA_VISIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_amount  = 512 # The number of tokens to which the tokenized text is truncated / padded.\n",
    "stats_cap          = 500 # Max value that the feature can take. Is NOT applicable to Betty numbers.\n",
    "    \n",
    "layers_of_interest = [i for i in range(12)]  # Layers for which attention matrices and features on them are \n",
    "                                             # calculated. For calculating features on all layers, leave it be\n",
    "                                             # [i for i in range(12)].\n",
    "stats_name = \"s_e_v_c_b0b1\" # The set of topological features that will be count (see explanation below)\n",
    "\n",
    "thresholds_array = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75] # The set of thresholds\n",
    "thrs = len(thresholds_array)                           # (\"t\" in the paper)\n",
    "\n",
    "model_path = \"bert-base-uncased\"  \n",
    "tokenizer_path = \"bert-base-uncased\"  \n",
    "\n",
    "# You can use either standard or fine-tuned BERT. If you want to use fine-tuned BERT to your current task, save the\n",
    "# model and the tokenizer with the commands tokenizer.save_pretrained(output_dir); \n",
    "# bert_classifier.save_pretrained(output_dir) into the same directory and insert the path to it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/sha43/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264b28c9de6a4ea98bfc300016e5f53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "x = 0\n",
    "sentences = []\n",
    "for elem in imdb[\"train\"]:\n",
    "    sentences.append(elem[\"text\"])\n",
    "sentences = np.array(sentences)\n",
    "print(sentences.shape)\n",
    "# print(\"Average amount of words in example:\", \\\n",
    "#       np.mean(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), sentences)))))\n",
    "# print(\"Max. amount of words in example:\", \\\n",
    "#       np.max(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), sentences)))))\n",
    "# print(\"Min. amount of words in example:\", \\\n",
    "#       np.min(list(map(len, map(lambda x: re.sub('\\w', ' ', x).split(\" \"), sentences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"sentence\": sentences[:2048]})\n",
    "sentences = data['sentence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_length(batch_texts):\n",
    "    inputs = tokenizer.batch_encode_plus(batch_texts,\n",
    "       return_tensors='pt',\n",
    "       add_special_tokens=True,\n",
    "       max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "       pad_to_max_length=True,         # Pad sentence to max length\n",
    "       truncation=True\n",
    "    )\n",
    "    inputs = inputs['input_ids'].numpy()\n",
    "    n_tokens = []\n",
    "    print(\"Counting lens\")\n",
    "    for i in tqdm(range(inputs.shape[0])):\n",
    "        ids = np.argwhere(inputs[i] == tokenizer.pad_token_id)\n",
    "        if not len(ids):\n",
    "            n_tokens.append(MAX_LEN)\n",
    "        else:\n",
    "            n_tokens.append(ids[0, 0])\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_tokens_amount\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16df36043e424e279560b3e04ca4096d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['tokenizer_length'] = get_token_length(data['sentence'].values)\n",
    "ntokens_array = data['tokenizer_length'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 2 # batch size\n",
    "number_of_batches = ceil(len(data['sentence']) / batch_size)\n",
    "DUMP_SIZE = 64 # number of batches to be dumped\n",
    "batched_sentences = np.array_split(data['sentence'].values, number_of_batches)\n",
    "adj_matricies = []\n",
    "adj_filenames = []\n",
    "assert number_of_batches == len(batched_sentences) # sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "MAX_LEN = max_tokens_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is calculated in \"f(v)\". You can add any other function from the array with vertex degrees.\n",
    "\n",
    "def function_for_v(list_of_v_degrees_of_graph):\n",
    "    return sum(map(lambda x: np.sqrt(x*x), list_of_v_degrees_of_graph))\n",
    "\n",
    "def split_matricies_and_lengths(adj_matricies, ntokens_array, num_of_workers):\n",
    "    splitted_adj_matricies = np.array_split(adj_matricies, num_of_workers)\n",
    "    splitted_ntokens = np.array_split(ntokens_array, num_of_workers)\n",
    "    print([len(elem) for elem in splitted_adj_matricies])\n",
    "    print([len(elem) for elem in splitted_ntokens])\n",
    "    assert all([len(m)==len(n) for m, n in zip(splitted_adj_matricies, splitted_ntokens)]), \"Split is not valid!\"\n",
    "    return zip(splitted_adj_matricies, splitted_ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_of_workers = 32\n",
    "pool = Pool(num_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:   5%|▌         | 7/128 [00:17<04:36,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  12%|█▏        | 15/128 [05:08<16:43,  8.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  18%|█▊        | 23/128 [09:44<15:49,  9.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  24%|██▍       | 31/128 [14:22<14:44,  9.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  30%|███       | 39/128 [19:42<15:04, 10.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  37%|███▋      | 47/128 [24:44<13:12,  9.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  43%|████▎     | 55/128 [30:20<12:38, 10.38s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  49%|████▉     | 63/128 [35:22<10:36,  9.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  55%|█████▌    | 71/128 [39:53<08:22,  8.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  62%|██████▏   | 79/128 [44:43<07:33,  9.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  68%|██████▊   | 87/128 [49:51<06:44,  9.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  74%|███████▍  | 95/128 [54:49<05:19,  9.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  80%|████████  | 103/128 [1:00:11<04:11, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  87%|████████▋ | 111/128 [1:04:49<02:32,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  93%|█████████▎| 119/128 [1:09:44<01:24,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc:  99%|█████████▉| 127/128 [1:15:07<00:10, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 12, 12, 512, 512)\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights calc: 100%|██████████| 128/128 [1:20:18<00:00, 37.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring 0 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = [f\"./finetuned-scrumbled-wikitext2-3e-4/checkpoint-{i}\" for i in range(80, 40000, 80)]\n",
    "models = [\"bert-base-uncased\"] + [f\"./finetuned-scrumbled-wikitext2-3e-4/checkpoint-{i}\" for i in range(80, 40000, 80)]\n",
    "\n",
    "for model_name in models:\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, output_attentions=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    stats_tuple_lists_array = []\n",
    "    adj_matricies = []\n",
    "    for i in tqdm(range(number_of_batches), desc=\"Weights calc\"): \n",
    "        attention_w = grab_attention_weights(model, tokenizer, batched_sentences[i], max_tokens_amount, device)\n",
    "        # sample X layer X head X n_token X n_token\n",
    "        adj_matricies.append(attention_w)\n",
    "        if (i+1) % DUMP_SIZE == 0: # dumping\n",
    "            adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "            adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "            batch_n = (i - 1) // DUMP_SIZE\n",
    "            ntokens = ntokens_array[batch_n*batch_size*DUMP_SIZE : (batch_n+1)*batch_size*DUMP_SIZE]\n",
    "            splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers)\n",
    "            args = [(m, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap) for m, ntokens in splitted]\n",
    "            stats_tuple_lists_array_part = pool.starmap(\n",
    "                count_top_stats, args\n",
    "            )\n",
    "            stats_tuple_lists_array.append(np.concatenate([_ for _ in stats_tuple_lists_array_part], axis=3))\n",
    "            adj_matricies = []\n",
    "    print(f\"Ignoring {len(adj_matricies)} sentences\")\n",
    "    stats_tuple_lists_array = np.concatenate(stats_tuple_lists_array, axis=3)\n",
    "    with open(f\"attention_features/{model_name.replace('./', '').replace('/', '_')}\", \"wb\") as fout:\n",
    "        np.save(fout, stats_tuple_lists_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46242d4576570c8c82d7fb56e00e670e22fb52831ef677247d21ad598b3c01cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
